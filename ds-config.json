{
  "train_batch_size": 64, 

  "gradient_accumulation_steps": 1, 

  "bf16": {
    "enabled": true
  },

  "zero_optimization": {
    "stage": 0
  },
  
  "tensor_parallel": {
    "tp_size": 2
  },

  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 0.0002
    }
  },

  "fp16": {
    "enabled": false
  },

  "wall_clock_breakdown": false
}
